# Emergent Tokens

How does a character level transformer reliably learn to make words, as opposed to any other N-gram? Some simple mechanistic interpretability experiments to find out how.

Work in progress.